{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e6f3a45",
   "metadata": {},
   "source": [
    "Dieses Notebook untersucht die Performance des Modells, wenn nur Artefakte einer Klasse als Artefakt gelten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f667146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling files in ../../../../tuar_processed: 0it [00:00, ?it/s]\n",
      "Labeling files in ../../../../tuar_processed/train: 100%|██████████| 320/320 [00:29<00:00, 10.93it/s]\n",
      "Labeling files in ../../../../tuar_processed/val: 100%|██████████| 98/98 [00:08<00:00, 11.53it/s]\n",
      "Labeling files in ../../../../tuar_processed/test: 100%|██████████| 118/118 [00:10<00:00, 11.31it/s]\n",
      "Labeling files in ../../../../tuar_processed: 0it [00:00, ?it/s]\n",
      "Labeling files in ../../../../tuar_processed/train: 100%|██████████| 320/320 [00:29<00:00, 10.83it/s]\n",
      "Labeling files in ../../../../tuar_processed/val: 100%|██████████| 98/98 [00:07<00:00, 13.68it/s]\n",
      "Labeling files in ../../../../tuar_processed/test: 100%|██████████| 118/118 [00:09<00:00, 12.88it/s]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from utils.labeling import label_all_files\n",
    "\n",
    "window_size_sec = 1.0\n",
    "window_overlap = 0.0\n",
    "overlap_treshold = 0.3\n",
    "dir_path = \"../../../../tuar_processed\"\n",
    "hdf5_path = \"./features/features.hdf5\"\n",
    "classes = ['eyem', 'musc']\n",
    "\n",
    "for artifact_class in classes:\n",
    "    label_generator = label_all_files(dir_path, window_size_sec, window_overlap, overlap_treshold, [artifact_class])\n",
    "\n",
    "    with h5py.File(hdf5_path, 'a') as hdf5_file:\n",
    "        for session, label_dict in label_generator:\n",
    "            session_group = hdf5_file.require_group(session)\n",
    "\n",
    "            for channel, (data, labels) in label_dict.items():\n",
    "                channel_group = session_group.require_group(channel)\n",
    "\n",
    "                if f'labels_{artifact_class}' in channel_group:\n",
    "                    del channel_group[f'labels_{artifact_class}']\n",
    "\n",
    "                channel_group.create_dataset(f'labels_{artifact_class}', data=labels, compression=\"gzip\", shuffle=True, chunks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef839080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File('features/features.hdf5') as f:\n",
    "    print(f['aaaaapas_s005_t000']['FP2-F4']['labels_eyem'][579:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400f5633",
   "metadata": {},
   "source": [
    "Scheint geklappt zu haben. Nun wird ein Modell auf diesen Daten trainiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbdcbab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features and labels for sessions: 100%|██████████| 268/268 [00:06<00:00, 39.96it/s]\n",
      "Extracting features and labels for sessions: 100%|██████████| 268/268 [00:01<00:00, 149.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report on validation set:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96   1129523\n",
      "           1       0.24      0.30      0.27     52603\n",
      "\n",
      "    accuracy                           0.93   1182126\n",
      "   macro avg       0.60      0.63      0.61   1182126\n",
      "weighted avg       0.93      0.93      0.93   1182126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.training import get_features_and_labels\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "label = 'labels_eyem'\n",
    "split_val = 'val'\n",
    "split_train = 'train'\n",
    "feature_file = \"./features/features.hdf5\"\n",
    "data_split_file = \"./data_split.yaml\"\n",
    "features = ['mean', 'variance', 'std', 'ptp_amp', 'kurtosis', 'quantile', 'pow_freq_bands', 'hurst_exp', 'line_length',\n",
    "            'rms', 'higuchi_fd', 'spect_entropy', 'svd_entropy', 'teager_kaiser_energy', 'wavelet_coef_energy',\n",
    "            'zero_crossings']\n",
    "\n",
    "random_state = 42\n",
    "max_depth = 25\n",
    "class_weight = 'balanced'\n",
    "model_save_path = f'./models/{datetime.now().strftime('%d-%m-%y %H-%M-%S')}.joblib'\n",
    "\n",
    "X_train, y_train = get_features_and_labels(feature_file, features, split_train, data_split_file, label)\n",
    "X_val, y_val = get_features_and_labels(feature_file, features, split_val, data_split_file, label)\n",
    "clf = RandomForestClassifier(n_jobs=-1, class_weight=class_weight, max_depth=max_depth, random_state=random_state)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "joblib.dump(clf, model_save_path)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "print(f'Classification report on validation set:\\n\\n{classification_report(y_val, y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "729132d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features and labels for sessions: 100%|██████████| 268/268 [00:05<00:00, 53.38it/s]\n",
      "Extracting features and labels for sessions: 100%|██████████| 268/268 [00:01<00:00, 179.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report on validation set:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.90      0.94   1118489\n",
      "           1       0.29      0.70      0.41     63637\n",
      "\n",
      "    accuracy                           0.89   1182126\n",
      "   macro avg       0.64      0.80      0.67   1182126\n",
      "weighted avg       0.94      0.89      0.91   1182126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.training import get_features_and_labels\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "label = 'labels_musc'\n",
    "split_val = 'val'\n",
    "split_train = 'train'\n",
    "feature_file = \"./features/features.hdf5\"\n",
    "data_split_file = \"./data_split.yaml\"\n",
    "features = ['mean', 'variance', 'std', 'ptp_amp', 'kurtosis', 'quantile', 'pow_freq_bands', 'hurst_exp', 'line_length',\n",
    "            'rms', 'higuchi_fd', 'spect_entropy', 'svd_entropy', 'teager_kaiser_energy', 'wavelet_coef_energy',\n",
    "            'zero_crossings']\n",
    "\n",
    "random_state = 42\n",
    "max_depth = 25\n",
    "class_weight = 'balanced'\n",
    "model_save_path = f'./models/{datetime.now().strftime('%d-%m-%y %H-%M-%S')}.joblib'\n",
    "\n",
    "X_train, y_train = get_features_and_labels(feature_file, features, split_train, data_split_file, label)\n",
    "X_val, y_val = get_features_and_labels(feature_file, features, split_val, data_split_file, label)\n",
    "clf = RandomForestClassifier(n_jobs=-1, class_weight=class_weight, max_depth=max_depth, random_state=random_state)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "joblib.dump(clf, model_save_path)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "print(f'Classification report on validation set:\\n\\n{classification_report(y_val, y_pred)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "artifact-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
